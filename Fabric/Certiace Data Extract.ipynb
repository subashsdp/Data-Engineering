{"cells":[{"cell_type":"code","source":["from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import pandas as pd"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"643d3ab6-c443-4215-b663-c63dab43e57f","normalized_state":"finished","queued_time":"2025-02-10T10:36:03.7891767Z","session_start_time":null,"execution_start_time":"2025-02-10T10:36:03.9304558Z","execution_finish_time":"2025-02-10T10:36:04.1866831Z","parent_msg_id":"6f34c02c-7fb8-419b-b118-9d3c91aa40ce"},"text/plain":"StatementMeta(, 643d3ab6-c443-4215-b663-c63dab43e57f, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb4deb4d-ac41-40d8-ba8c-c15533318b32"},{"cell_type":"code","source":["# https://certiace.com/dp700_questions_formatted.json\n","df_certi = []\n","df_certi = pd.read_json(\"https://certiace.com/dp700_questions_formatted.json\")\n","df_final = []\n","x=1\n","for cet in df_certi.questions:    \n","    for cert_a in cet:\n","        df_final.append({\n","                        'sno': x,\n","                        'question':cert_a['question'],\n","                        'option1':cert_a['options'][0],\n","                        'option2':cert_a['options'][1],\n","                        'option3':cert_a['options'][2],\n","                        'option4':cert_a['options'][3],\n","                        'correctAnswer':cert_a['correctAnswer'],\n","                        'explanation':cert_a['explanation'],\n","                        'sourceUrl':cert_a['sourceUrl']\n","                         })\n","        x = x+1\n","display(df_final)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"643d3ab6-c443-4215-b663-c63dab43e57f","normalized_state":"finished","queued_time":"2025-02-10T10:36:03.9079539Z","session_start_time":null,"execution_start_time":"2025-02-10T10:36:04.3153743Z","execution_finish_time":"2025-02-10T10:36:06.7964553Z","parent_msg_id":"50bfae77-a870-4ab7-8036-a75082dd8bbd"},"text/plain":"StatementMeta(, 643d3ab6-c443-4215-b663-c63dab43e57f, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"28dd71ca-f6d9-4590-8e0d-1ac9941017df","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 28dd71ca-f6d9-4590-8e0d-1ac9941017df)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"id":"dc0fedfe-681d-4668-9df3-05e98a12d525"},{"cell_type":"code","source":["df_finalSpark = spark.createDataFrame(df_final)\n","df_finalSpark.write.format(\"delta\")\\\n","            .mode(\"overwrite\")\\\n","            .saveAsTable(\"BronzeLH.ex_df_certiace\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"643d3ab6-c443-4215-b663-c63dab43e57f","normalized_state":"finished","queued_time":"2025-02-10T10:36:04.0436337Z","session_start_time":null,"execution_start_time":"2025-02-10T10:36:06.9380702Z","execution_finish_time":"2025-02-10T10:36:11.9044848Z","parent_msg_id":"11955fea-6257-4b2c-be4c-78a489c70bbc"},"text/plain":"StatementMeta(, 643d3ab6-c443-4215-b663-c63dab43e57f, 21, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc886bfd-5a5d-434c-87e0-34518c1966a7"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM BronzeLH.ex_df_certiace LIMIT 1000\")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"643d3ab6-c443-4215-b663-c63dab43e57f","normalized_state":"finished","queued_time":"2025-02-10T10:36:04.1766229Z","session_start_time":null,"execution_start_time":"2025-02-10T10:36:12.0360718Z","execution_finish_time":"2025-02-10T10:36:14.5098675Z","parent_msg_id":"f5589721-7a11-4f72-ad3c-f74bef72cf6e"},"text/plain":"StatementMeta(, 643d3ab6-c443-4215-b663-c63dab43e57f, 22, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"78caf934-f8aa-4a0f-bc5a-f392ed79b7b3","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 78caf934-f8aa-4a0f-bc5a-f392ed79b7b3)"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"706fa386-d8e1-4548-b843-df1203ea1487"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{"28dd71ca-f6d9-4590-8e0d-1ac9941017df":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"Dataflows Gen2 are primarily used for transforming data within Microsoft Fabric, allowing for efficient data manipulation before it is stored or analyzed.","2":"Data ingestion","3":"Data transformation","4":"Data storage","5":"Data visualization","6":"What is the primary purpose of using Dataflows Gen2 in Microsoft Fabric?","7":"1","8":"https://learn.microsoft.com/en-us/training/modules/use-dataflow-gen-2-fabric/4-dataflow-pipeline"},{"0":"0","1":"Pipelines consist of two main types of activities: data transformation activities, which handle data transfer and transformation, and control flow activities, which manage the execution logic of the pipeline.","2":"Data transformation activities and Control flow activities","3":"Data processing activities and Control flow activities","4":"Data ingestion activities and Data storage activities","5":"Data extraction activities and Data transformation activities","6":"What are the two broad categories of activities in a pipeline?","7":"2","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/2-understand-fabric-pipeline"},{"0":"0","1":"Parameters allow pipelines to accept dynamic values (e.g., folder names) at runtime, enabling reusability.","2":"Parameters","3":"Variables","4":"Triggers","5":"Hardcoded folder names","6":"A team wants to reuse a pipeline to load data into different lakehouse folders dynamically. Which pipeline feature should they use?","7":"3","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/2-understand-fabric-pipeline"},{"0":"1","1":"The Copy Data activity is primarily used to ingest data from an external source into a lakehouse file or table without applying transformations.","2":"To apply transformations to data during ingestion","3":"To ingest data from an external source into a lakehouse file or table","4":"To delete existing data before copying new data","5":"To merge data from multiple sources","6":"What is the primary use of the Copy Data activity in a data pipeline?","7":"4","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"2","1":"A Data Flow activity should be used when you need to apply transformations to the data as it is ingested or merge data from multiple sources.","2":"When you need to copy data directly between a source and destination","3":"When you want to import raw data without transformations","4":"When you need to apply transformations to the data as it is ingested","5":"When you are using a graphical tool to configure the data source","6":"When should you consider using a Data Flow activity instead of a Copy Data activity?","7":"5","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"1","1":"Data Flow activities use Power Query for defining transformations.","2":"Azure Portal","3":"Power Query","4":"Spark Notebook","5":"Graphical Copy Data tool","6":"What tool is used to define transformation steps for a Data Flow activity?","7":"6","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"1","1":"The lambda architecture combines the periodic loading of batch data for historical analysis with the ingestion of data streams for real-time analysis.","2":"Data warehouse architecture","3":"Lambda architecture","4":"Event-driven architecture","5":"Microservices architecture","6":"Which architecture combines batch data loading with real-time data ingestion?","7":"7","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"2","1":"Eventstream in Microsoft Fabric is used to ingest real-time data from diverse sources as a continuous stream.","2":"Eventhouse","3":"Power BI Dataset","4":"Eventstream","5":"KQL Database","6":"Which Microsoft Fabric component is specifically designed for ingesting continuous real-time data?","7":"8","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"1","1":"Activator enables defining automated alerts and actions when specific conditions occur in real-time data streams.","2":"Automate data transformation pipelines","3":"Trigger alerts based on streaming data thresholds","4":"Generate batch analytics reports","5":"Manage data warehouse indexing","6":"What is the primary purpose of Activator in Microsoft Fabric's Real-Time Intelligence?","7":"9","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The Join transformation is designed to combine data from two streams using a matching condition, unlike Union, which merges streams with shared fields.","2":"Join","3":"Union","4":"Group by","5":"Expand","6":"Which transformation in Eventstreams is used to combine data from two streams based on a matching condition between them?","7":"10","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"A KQL database is specifically designed as a real-time optimized data store that hosts a collection of tables, stored functions, materialized views, and shortcuts.","2":"A data store optimized for batch processing that hosts static tables","3":"A real-time optimized data store that hosts tables and functions","4":"A query language tool for processing historical data","5":"A storage system focused only on table management and data ingestion","6":"What is a KQL database?","7":"11","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"'take' is an operator in KQL that retrieves a specified number of rows from a table.","2":"Filters data based on a condition","3":"Retrieves a specified number of rows from a table","4":"Aggregates data","5":"Joins two tables together","6":"What does the 'take' operator do in a KQL query?","7":"12","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"The 'SELECT TOP 10 * FROM stock' is the SQL equivalent of the KQL 'take 10' operator.","2":"SELECT * FROM stock LIMIT 10","3":"SELECT TOP 10 * FROM stock","4":"FETCH FIRST 10 ROWS FROM stock","5":"SELECT * FROM stock WHERE ROWNUM <= 10","6":"Which T-SQL statement is equivalent to the KQL query 'stock | take 10'?","7":"13","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"2","1":"Transformations can perform a variety of operations including filtering, joining, aggregating, and grouping, as well as temporal windowing functions.","2":"Only filtering data","3":"Only aggregating data","4":"Filtering, joining, aggregating, and grouping","5":"Storing data in destinations","6":"What type of operations can transformations in an eventstream perform?","7":"14","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/2-eventstream-components"},{"0":"2","1":"The Derived Stream destination is created after applying stream operations like Filter or Manage Fields to an eventstream, representing the altered default stream.","2":"To store raw event data in a KQL database","3":"To preprocess events before ingestion into a lakehouse","4":"To represent the altered default stream after applying stream operations","5":"To trigger automated actions based on streaming data values","6":"What is the primary purpose of the Derived Stream destination in Microsoft Fabric Eventstreams?","7":"15","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/3-setup-eventstreams"},{"0":"2","1":"Session windows group events into variable and non-overlapping intervals based on a gap of inactivity.","2":"Tumbling","3":"Sliding","4":"Session","5":"Hopping","6":"Which window type groups events based on a gap of inactivity?","7":"16","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"2","1":"The 'project' keyword is used in KQL to specify which columns to retrieve from a dataset, helping to optimize performance by minimizing the data returned.","2":"filter","3":"select","4":"project","5":"retrieve","6":"What keyword should you use to retrieve only specific columns in a KQL query?","7":"17","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/3-kql-best-practices"},{"0":"1","1":"The ingestion_time() function filters data based on when it was loaded into the table, which is essential when the data itself lacks a timestamp.","2":"Automotive | where pickup_datetime > ago(1h)","3":"Automotive | where ingestion_time() > ago(1h)","4":"Automotive | where current_utc_time() - ingestion_time() < 1h","5":"Automotive | where timestamp > ago(1h)","6":"Which KQL query correctly retrieves events ingested into the Automotive table within the past hour when the data lacks a timestamp?","7":"18","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/3-kql-best-practices"},{"0":"1","1":"Materialized views created without the backfill option are populated incrementally as new data is ingested, rather than processing existing data.","2":"The view is immediately populated with all existing data.","3":"The view is populated incrementally as new data is ingested.","4":"Existing data is deleted from the source table.","5":"The view creation fails unless backfill is enabled.","6":"How does a materialized view populate itself when created without the backfill option?","7":"19","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/4-advanced-features"},{"0":"2","1":"Materialized views are designed to maintain pre-aggregated datasets efficiently, which is ideal for frequent queries requiring summarized data.","2":"Reusing a parameterized query with variable inputs","3":"Creating an on-demand summary of historical data","4":"Automatically maintaining a pre-aggregated dataset for frequent queries","5":"Performing real-time transformations during data ingestion","6":"Which scenario best justifies using a materialized view instead of a stored function?","7":"20","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/4-advanced-features"},{"0":"0","1":"OneLake is designed to provide a single, integrated storage environment for all analytics engines in Fabric, eliminating the need to move or copy data between systems.","2":"To provide a unified storage solution for all analytics engines in Fabric","3":"To act as a data integration tool for moving data between systems","4":"To serve as a standalone data warehouse solution","5":"To manage compute resources for Fabric workspaces","6":"What is the primary purpose of OneLake in Microsoft Fabric?","7":"21","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/2-explore-analytics-fabric"},{"0":"1","1":"Shortcuts in OneLake enable users to create embedded references to existing data sources, facilitating easy access without duplication.","2":"Data lineage","3":"Shortcuts","4":"Dataflows","5":"Workspaces","6":"What feature of OneLake allows users to reference existing cloud data without copying it?","7":"22","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/2-explore-analytics-fabric"},{"0":"1","1":"A workspace serves as a collaborative environment for creating and managing items like lakehouses, warehouses, and reports.","2":"To store data permanently","3":"To create and manage collaborative items","4":"To perform data analysis","5":"To configure Azure settings","6":"What is the primary purpose of a workspace in Microsoft Fabric?","7":"23","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/4-use-fabric"},{"0":"1","1":"Lakehouses support ACID transactions, which are essential for maintaining data consistency and integrity.","2":"Schema-on-write","3":"ACID transactions through Delta Lake formatted tables","4":"Only read-only access","5":"Data replication across multiple regions","6":"What feature do lakehouses support to ensure data consistency and integrity?","7":"24","8":"https://learn.microsoft.com/en-us/training/modules/get-started-lakehouses/2-fabric-lakehouse"},{"0":"1","1":"Access to a Fabric lakehouse is managed through workspace roles for collaborators and item-level sharing for read-only needs.","2":"Through individual user accounts only","3":"Using workspace roles and item-level sharing","4":"Exclusively via API keys","5":"Through Azure Active Directory groups only","6":"How is access to a Fabric lakehouse typically managed?","7":"25","8":"https://learn.microsoft.com/en-us/training/modules/get-started-lakehouses/2-fabric-lakehouse"},{"0":"1","1":"The partitionBy method is used to optimize performance by partitioning the data into separate folders based on the specified column values.","2":"To specify the file format","3":"To optimize performance by partitioning data","4":"To define the schema","5":"To overwrite existing files","6":"What is the purpose of the partitionBy method when saving a DataFrame?","7":"26","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/4-dataframe"},{"0":"2","1":"The method df.write.format('delta').saveAsTable is specifically used to save a dataframe as a new table in Spark SQL.","2":"spark.catalog.createTable","3":"df.createOrReplaceTempView","4":"df.write.format('delta').saveAsTable","5":"spark.catalog.createExternalTable","6":"Which method is used to save a dataframe as a new table in Spark SQL?","7":"27","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql"},{"0":"1","1":"Deleting an external table only removes the metadata from the catalog, while the underlying data remains intact.","2":"Both the metadata and data files are deleted","3":"Only the metadata is deleted from the catalog, while the data files remain intact in their storage location","4":"The metadata is archived and the data files are deleted","5":"The metadata and data files are both archived, and can be restored later","6":"What happens when you delete an external table from Spark Catalog?","7":"28","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql"},{"0":"1","1":"Delta tables provide ACID transaction support, ensuring that data modifications are atomic, consistent, isolated, and durable.","2":"They only support batch data processing.","3":"They provide ACID transaction support.","4":"They can only store static data.","5":"They do not support data versioning.","6":"What is the primary benefit of Delta tables in Microsoft Fabric lakehouses?","7":"29","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/2-understand-delta-lake"},{"0":"1","1":"The _delta_log folder contains transaction logs in JSON format, which are used to track changes and ensure ACID compliance.","2":"To store Parquet data files","3":"To log transaction details in JSON format","4":"To store metadata for unstructured data","5":"To cache query results for faster access","6":"What is the purpose of the _delta_log folder in a Delta table?","7":"30","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/2-understand-delta-lake"},{"0":"1","1":"When a managed table is deleted, both the table definition and the underlying data files are deleted from the lakehouse.","2":"They are retained","3":"They are deleted","4":"They are archived","5":"They are moved to another location","6":"What happens to the underlying data files when a managed table is deleted in a Fabric lakehouse?","7":"31","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3-create-delta-tables"},{"0":"1","1":"OptimizeWrite helps in writing fewer larger files instead of many small files, addressing the small file problem.","2":"To delete old data files","3":"To reduce the number of small files written","4":"To partition data into subfolders","5":"To enable time travel for data","6":"What is the purpose of the OptimizeWrite function in Delta Lake?","7":"32","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3b-optimize-delta-tables"},{"0":"1","1":"The VACUUM command is used to remove old data files that are not referenced in the transaction log, helping to manage storage.","2":"It consolidates small files into larger ones","3":"It removes old data files that are no longer referenced","4":"It enables faster reads from the compute engines","5":"It partitions data into subfolders","6":"What does the VACUUM command do in Delta Lake?","7":"33","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3b-optimize-delta-tables"},{"0":"1","1":"The DESCRIBE HISTORY command is specifically used to view the history of transactions applied to a delta table.","2":"SHOW HISTORY","3":"DESCRIBE HISTORY","4":"VIEW HISTORY","5":"GET HISTORY","6":"Which command is used to view the history of changes made to a delta table?","7":"34","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/4-work-delta-data"},{"0":"0","1":"To retrieve data from a specific version of a delta table, you can use the versionAsOf option when reading the delta file location into a dataframe.","2":"Using the versionAsOf option","3":"Using the timestampAsOf option","4":"Using the version option","5":"Using the timeTravel option","6":"How can you retrieve data from a specific version of a delta table?","7":"35","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/4-work-delta-data"},{"0":"2","1":"The bronze layer serves as the initial landing zone for all data, where it is stored in its original format without any changes.","2":"To store data in a refined format","3":"To validate and clean the data","4":"To serve as the landing zone for raw data","5":"To aggregate data for analytics","6":"What is the primary purpose of the bronze layer in the medallion architecture?","7":"36","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/2-describe-medallion-architecture"},{"0":"1","1":"The medallion architecture ensures data reliability and consistency as it moves through different layers, improving data quality and making it easier to analyze.","2":"It replaces existing data models entirely","3":"It ensures data reliability and consistency as it moves through layers","4":"It eliminates the need for data transformation tools","5":"It restricts data access to a single team","6":"What is the primary benefit of using the medallion architecture in a lakehouse?","7":"37","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/2-describe-medallion-architecture"},{"0":"2","1":"The Gold layer is modeled in a star schema, which is optimized for reporting.","2":"Bronze","3":"Silver","4":"Gold","5":"Raw","6":"Which layer in the medallion architecture is typically modeled in a star schema?","7":"38","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/3-implement-medallion-archecture-fabric"},{"0":"1","1":"The SQL analytics endpoint allows users to write queries, manage the semantic model, and utilize a visual query experience.","2":"To create new data layers in the medallion architecture","3":"To write queries, manage the semantic model, and query data visually","4":"To generate cleansed data for third-party applications","5":"To connect directly to Power BI without a semantic model","6":"What is the purpose of the SQL analytics endpoint in Fabric?","7":"39","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/4-query-report-data"},{"0":"2","1":"Direct Lake mode caches often-used data and refreshes it as required, enhancing performance and ensuring up-to-date access to lakehouse data.","2":"It allows for real-time data processing without caching","3":"It provides a static view of the data in the lakehouse","4":"It caches frequently used data and refreshes it as needed","5":"It requires manual updates to the semantic model","6":"How does Direct Lake mode benefit data analysts using Power BI?","7":"40","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/4-query-report-data"},{"0":"1","1":"A key characteristic of a data stream in real-time analytics is that it is unbounded, meaning data is continuously added to the stream without a predefined limit.","2":"Data records are fixed and do not change","3":"Data is added to the stream perpetually","4":"Data streams are only processed once a day","5":"Data records do not include time-based information","6":"Which characteristic describes a data stream in real-time analytics?","7":"41","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The results of streaming data processing can be used to support real-time (or near real-time) automation or visualization, enabling immediate insights and actions.","2":"To support real-time automation or visualization.","3":"To archive data in a cold storage solution.","4":"To perform batch processing of historical data.","5":"To generate static reports for regulatory compliance.","6":"How can the results of streaming data processing be used?","7":"42","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The Microsoft Fabric Real-Time Intelligence solution is designed to provide an end-to-end streaming solution for real-time data analysis, including managing real-time data sources.","2":"To provide a centralized place for managing real-time data sources","3":"To create and manage virtual machines in Azure","4":"To deploy highly available web applications","5":"To store and analyze batch data in a data warehouse","6":"What is the primary purpose of the Microsoft Fabric Real-Time Intelligence solution?","7":"43","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3-describe-kusto-databases-tables"},{"0":"2","1":"The Eventhouse destination is specifically meant for ingesting real-time event data, allowing for analysis using Kusto Query Language.","2":"Lakehouse","3":"Derived stream","4":"Eventhouse","5":"Fabric Activator","6":"Which Eventstreams destination allows direct KQL analysis of raw event data?","7":"44","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"The Union transformation is used to connect multiple nodes and combine events with shared fields into one table.","2":"Groups array values into a single row","3":"Creates a new row for each value within an array","4":"Removes array values from the stream","5":"Combines multiple arrays into one array","6":"What does the Expand transformation do in eventstreams?","7":"45","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"A KQL database is designed to host tables and functions that facilitate real-time data analysis.","2":"Storing static data","3":"Hosting a collection of tables and functions for real-time data analysis","4":"Performing batch processing of data","5":"Creating visual reports","6":"What is a KQL database primarily used for in an eventhouse?","7":"46","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"Rules are crucial in Activator as they define the conditions under which actions are triggered based on property values.","2":"They define the structure of events","3":"They set conditions for triggering actions","4":"They represent business objects","5":"They map properties to event data","6":"What role do Rules play in Activator?","7":"47","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4c-activator"},{"0":"1","1":"Microsoft Fabric eventstreams are designed to facilitate the movement of event data from various sources to different destinations, functioning like a conveyor belt.","2":"To store data in a database","3":"To create a pipeline of events from sources to destinations","4":"To manage infrastructure for applications","5":"To write complex code for data processing","6":"What is the primary function of Microsoft Fabric eventstreams?","7":"48","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/2-eventstream-components"},{"0":"2","1":"Tumbling windows are characterized by dividing events into fixed and nonoverlapping intervals based on their arrival time.","2":"Sliding windows","3":"Session windows","4":"Tumbling windows","5":"Hopping windows","6":"Which window type divides incoming events into fixed and nonoverlapping intervals?","7":"49","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"1","1":"The Union transformation is used to connect multiple nodes and combine events that share the same fields, effectively merging data streams.","2":"To filter events based on specific criteria","3":"To connect two or more nodes and combine events with shared fields","4":"To calculate aggregations over a specified time period","5":"To create new rows for each value in an array","6":"What is the function of the Union transformation in eventstream processing?","7":"50","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"2","1":"The grouping key in the Group by transformation specifies one or more columns in the event data to group events by, such as sensor ID or item category.","2":"To define the length of each window interval","3":"To shift the start and end of each window interval","4":"To specify the columns to group events by","5":"To filter events based on a condition","6":"What is the purpose of the grouping key in the Group by transformation?","7":"51","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"0","1":"Creating an eventhouse requires a workspace with a Fabric capacity that supports the Real-Time Intelligence Fabric capability.","2":"A Fabric capacity that supports Real-Time Intelligence Fabric capability","3":"A KQL database already created","4":"A sample dataset imported","5":"An Azure Event Hub configured","6":"What is required to create an eventhouse in Microsoft Fabric?","7":"52","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"The KQL query 'Automotive | take 100' is specifically designed to retrieve a sample of 100 rows from the Automotive table.","2":"SELECT * FROM Automotive LIMIT 100","3":"Automotive | take 100","4":"Automotive | limit 100","5":"SELECT TOP 100 * FROM Automotive","6":"Which KQL query retrieves the first 100 rows from the Automotive table?","7":"53","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"KQL Querysets are designed to simplify query development by providing sample syntax and coding utilities for users.","2":"To store KQL databases","3":"To simplify query development with sample syntax and coding utilities","4":"To visualize query results in Power BI","5":"To import data from static locations","6":"What is the purpose of KQL Querysets in eventhouses?","7":"54","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"A real-time dashboard requires a source of real-time data to function effectively.","2":"A static data source","3":"A source of real-time data","4":"A SQL database","5":"A data warehouse","6":"What is required to create a real-time dashboard in Microsoft Fabric?","7":"55","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/2-get-started-with-real-time-dashboards"},{"0":"1","1":"The 'Dashboard editor's identity' scheme uses the identity of the user who created the dashboard to access data.","2":"Pass-through identity","3":"Dashboard editor's identity","4":"User-specific identity","5":"Admin identity","6":"Which authorization scheme allows the dashboard to access data using the identity of the user who created the dashboard?","7":"56","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/2-get-started-with-real-time-dashboards"},{"0":"0","1":"A base query allows for the retrieval of a general set of records that can be used across multiple tiles, making the dashboard more maintainable.","2":"To retrieve a general set of records relevant for multiple tiles","3":"To create a static dashboard without any data updates","4":"To limit the number of tiles on the dashboard","5":"To enhance the visual appeal of the dashboard","6":"What is the purpose of defining a base query in a real-time dashboard?","7":"57","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"0","1":"Parameters provide flexibility by enabling users to filter the data displayed in the tiles based on their selections.","2":"By allowing users to filter data displayed in the tiles","3":"By automatically refreshing the dashboard every minute","4":"By limiting the number of pages in the dashboard","5":"By providing a static view of the data","6":"How can parameters enhance a real-time dashboard?","7":"58","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"0","1":"The auto refresh feature ensures that the dashboard data is updated automatically, eliminating the need for manual refresh actions.","2":"It automatically updates dashboard data without manual intervention","3":"It prevents any changes to the dashboard layout","4":"It allows users to set a maximum refresh rate","5":"It disables all other dashboard features","6":"What does the 'auto refresh' feature do in a real-time dashboard?","7":"59","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"1","1":"Clarity and simplicity are essential to ensure that dashboards are easy to understand and use, avoiding clutter.","2":"Complexity and Detail","3":"Clarity and Simplicity","4":"High Refresh Rates","5":"Limited Interactivity","6":"What is a key principle to follow when designing real-time dashboards in Microsoft Fabric?","7":"60","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/4-real-time-dashboards-best-practices"},{"0":"1","1":"The star schema is defined by its structure where a central fact table is directly connected to multiple dimension tables, facilitating efficient queries.","2":"Snowflake schema","3":"Star schema","4":"Galaxy schema","5":"Hybrid schema","6":"Which schema design is characterized by a fact table directly related to dimension tables?","7":"61","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/2-understand-data-warehouse"},{"0":"1","1":"Surrogate keys serve as unique identifiers for each row in a dimension table, ensuring consistency and accuracy within the data warehouse.","2":"To provide a natural identifier from the source system","3":"To uniquely identify each row in the dimension table","4":"To track changes in dimension attributes over time","5":"To aggregate data over temporal intervals","6":"What is the role of surrogate keys in dimension tables?","7":"62","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/2-understand-data-warehouse"},{"0":"2","1":"Zero-copy table clones allow for minimal storage costs because they reference the same underlying data files without duplicating them.","2":"Increased storage costs","3":"Faster data ingestion","4":"Minimal storage costs while referencing the same data","5":"Automatic data cleansing","6":"Which of the following is a benefit of using zero-copy table clones in a data warehouse?","7":"63","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/3-understand-data-warehouse-fabric"},{"0":"0","1":"A semantic model is designed to define relationships between tables, aggregation rules, and calculations for deriving insights from data.","2":"To define relationships and calculations for data insights","3":"To store raw data without any transformations","4":"To visualize data without any reporting tools","5":"To manage user permissions in the data warehouse","6":"What is the purpose of a semantic model in a data warehouse?","7":"64","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/5-model-data"},{"0":"2","1":"The sys.dm_exec_requests DMV provides details about each active request in a session, allowing for monitoring of ongoing operations.","2":"sys.dm_exec_connections","3":"sys.dm_exec_sessions","4":"sys.dm_exec_requests","5":"sys.dm_exec_queries","6":"Which dynamic management view (DMV) would you use to get information about active requests in a session?","7":"65","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/6-security-monitor"},{"0":"2","1":"The Read permission is essential for establishing a connection to the SQL analytics endpoint, as it allows the user to connect using the SQL connection string.","2":"ReadData","3":"ReadAll","4":"Read","5":"Write","6":"What permission must a user have at a minimum to connect to the SQL analytics endpoint?","7":"66","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/6-security-monitor"},{"0":"2","1":"Type 2 SCD adds new records for changes and keeps full history for a given natural key, allowing for historical analysis.","2":"Type 0 SCD","3":"Type 1 SCD","4":"Type 2 SCD","5":"Type 3 SCD","6":"Which type of slowly changing dimension (SCD) keeps full history for a given natural key?","7":"67","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/2-explore-data-load-strategies"},{"0":"1","1":"The REJECTED_ROW_LOCATION option allows for better error handling by storing rows that were not successfully imported.","2":"To specify the format of the source file","3":"To store rejected rows separately","4":"To skip header rows","5":"To define the target table","6":"What is the purpose of the REJECTED_ROW_LOCATION option in the COPY statement?","7":"68","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"2","1":"The INSERT...SELECT operation is used to insert data from one table into another without creating a new table.","2":"CREATE TABLE AS SELECT","3":"COPY INTO","4":"INSERT...SELECT","5":"BULK INSERT","6":"Which SQL operation is used to insert data from one table into another without creating a new table?","7":"69","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"1","1":"The three-part naming convention allows combining data from a warehouse and a lakehouse by referencing tables across these assets.","2":"COPY statement","3":"Three-part naming convention","4":"Shared Access Signature","5":"Storage Account Key","6":"Which feature allows combining data from a warehouse and a lakehouse in Microsoft Fabric?","7":"70","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"1","1":"The APPROX_COUNT_DISTINCT function is specifically designed to retrieve an approximate count of distinct values using the HyperLogLog algorithm, which is useful for large datasets where an exact count is not required.","2":"COUNT","3":"APPROX_COUNT_DISTINCT","4":"DISTINCT_COUNT","5":"ESTIMATE_COUNT","6":"Which T-SQL function is used to retrieve an approximate count of distinct values in a data warehouse?","7":"71","8":"https://learn.microsoft.com/en-us/training/modules/query-data-warehouse-microsoft-fabric/2-query-data"},{"0":"2","1":"The DENSE_RANK function assigns the same rank to rows with the same values and does not skip subsequent ranking positions, unlike RANK, which does skip positions after ties.","2":"ROW_NUMBER","3":"RANK","4":"DENSE_RANK","5":"NTILE","6":"Which T-SQL ranking function assigns the same rank to rows with the same values but does not skip subsequent ranking positions?","7":"72","8":"https://learn.microsoft.com/en-us/training/modules/query-data-warehouse-microsoft-fabric/2-query-data"},{"0":"1","1":"Throttling in Microsoft Fabric indicates that the processes require more capacity than is available within the constraints of the purchased capacity license.","2":"The system is running at optimal capacity","3":"The processes require more capacity than is available","4":"The data warehouse is being migrated to a new region","5":"The license has expired and needs renewal","6":"What does throttling in Microsoft Fabric indicate?","7":"73","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/02-capacity-metrics"},{"0":"1","1":"The sys.dm_exec_connections DMV specifically provides information regarding the connections to the data warehouse.","2":"Active requests","3":"Data warehouse connections","4":"Authenticated sessions","5":"Database transactions","6":"What does the sys.dm_exec_connections DMV return information about?","7":"74","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/03-dynamic-management-views"},{"0":"2","1":"The queryinsights.long_running_queries view is specifically designed to provide aggregated data regarding query execution time, including the median execution time (median_total_elapsed_time_ms).","2":"queryinsights.exec_requests_history","3":"queryinsights.frequently_run_queries","4":"queryinsights.long_running_queries","5":"A calculated field derived from queryinsights.exec_requests_history only","6":"Which queryinsights view directly provides the median execution time for SQL commands?","7":"75","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/04-query-insights"},{"0":"1","1":"The primary benefit of DDM is its ability to mask sensitive data in real time, ensuring that unauthorized users do not see the actual data.","2":"It permanently alters the data in the database","3":"It allows real-time masking of sensitive data","4":"It requires complex coding to implement","5":"It exposes all data to nonprivileged users","6":"What is the primary benefit of Dynamic Data Masking (DDM)?","7":"76","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"The email() function is specifically designed to expose the first letter of an email address while masking the rest.","2":"default()","3":"email()","4":"partial(prefix_padding, padding_string, suffix_padding)","5":"random(low, high)","6":"Which masking function would you use to partially hide an email address while exposing the first letter?","7":"77","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"The correct command to apply a masking rule to the PhoneNumber column is to use the partial() function with the specified parameters.","2":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'random(1000000000,9999999999)')","3":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXX-XXX-\",4)')","4":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'email()')","5":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'default()')","6":"You need to dynamically mask the PhoneNumber column to display only the last four digits, replacing the rest with XXX-XXX-. Which SQL command achieves this?","7":"78","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"Column-level security is designed to restrict access to sensitive data, ensuring that only authorized users can view specific columns.","2":"To enhance performance of queries","3":"To restrict access to sensitive data","4":"To simplify database management","5":"To allow all users to access all data","6":"What is the primary purpose of column-level security (CLS) in a data warehouse?","7":"79","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/4-implement-column-level-security"},{"0":"1","1":"Column-level security automatically adapts to changes in table structure, making it easier to manage than views which may require updates when the underlying structure changes.","2":"More flexible in defining permissions","3":"Automatically adapts to table structure changes","4":"Can provide row-level security","5":"Requires less maintenance","6":"What is a key advantage of using column-level security compared to views?","7":"80","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/4-implement-column-level-security"},{"0":"1","1":"Deployment pipelines facilitate the promotion of code changes through various environments, ensuring a structured deployment process.","2":"They automate the integration of code changes","3":"They allow for the promotion of code changes to different environments","4":"They manage version control for individual developers","5":"They provide a platform for manual testing of code","6":"What role do deployment pipelines play in the CI/CD process within Fabric?","7":"81","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/2-understand-cicd"},{"0":"1","1":"Continuous delivery follows continuous integration and involves deploying code to a staging environment for additional automated testing before production release.","2":"It automatically releases updates into production without any testing","3":"It occurs after continuous integration and involves deploying code to a staging environment for further testing","4":"It is the process of merging code changes from multiple developers into a single branch","5":"It focuses solely on the management of version control using Git","6":"Which of the following statements best describes continuous delivery?","7":"82","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/2-understand-cicd"},{"0":"1","1":"Deployment pipelines are designed to automate the movement of content through various stages of the development lifecycle, ensuring efficient updates and testing.","2":"To create user interfaces for applications","3":"To automate the movement of content through development stages","4":"To manage user permissions across environments","5":"To store data securely in the cloud","6":"What is the primary purpose of deployment pipelines in Microsoft Fabric?","7":"83","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/4-implement-deployment-pipelines"},{"0":"1","1":"Monitoring is essential for collecting data and metrics that help determine the health and operational status of a system.","2":"To enhance user interface design","3":"To collect system data and metrics for health assessment","4":"To automate data ingestion processes","5":"To create visual representations of data models","6":"What is the primary purpose of monitoring in Microsoft Fabric?","7":"84","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/2-understand-monitoring"},{"0":"1","1":"The Monitor hub is designed specifically to visualize and monitor various activities within Microsoft Fabric, making it easier to track performance and issues.","2":"To create new data pipelines","3":"To visualize and monitor Fabric activities","4":"To store data permanently","5":"To execute Spark jobs","6":"What is the primary function of the Monitor hub in Microsoft Fabric?","7":"85","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/3-use-monitor-hub"},{"0":"1","1":"Activator is designed to automate actions triggered by specific events in data streams.","2":"To store large volumes of data","3":"To automate actions based on events","4":"To visualize data in real-time dashboards","5":"To manage user access and permissions","6":"What is the primary function of Activator in Microsoft Fabric?","7":"86","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/4-take-action-with-activator"},{"0":"1","1":"Workspace roles in Fabric are used to distribute ownership and access policies within a workspace, enabling control over who can access and manage resources.","2":"To authenticate users via Microsoft Entra ID","3":"To distribute ownership and access policies within a workspace","4":"To apply granular permissions within a specific compute engine","5":"To restrict access to specific files or folders in OneLake","6":"In Fabric, what is the purpose of workspace roles?","7":"87","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/2-understand-fabric-security-model"},{"0":"2","1":"OneLake data access controls enable the restriction of access to specific files or folders within the lakehouse.","2":"Workspace roles","3":"Item permissions","4":"OneLake data access controls","5":"Granular engine permissions","6":"What feature allows access to data in the lakehouse to be restricted to specific files or folders?","7":"88","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/2-understand-fabric-security-model"},{"0":"2","1":"The Contributor role allows users to create Fabric items and modify content, which meets the requirements for the data engineer.","2":"Admin","3":"Member","4":"Contributor","5":"Viewer","6":"What role should be assigned to a data engineer who needs to create Fabric items and read data in a lakehouse?","7":"89","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"The GRANT command is specifically used in SQL to provide permissions to users or roles for accessing database objects.","2":"ALLOW","3":"GRANT","4":"PERMIT","5":"ENABLE","6":"What command is used to grant permissions to SQL objects using the SQL analytics endpoint?","7":"90","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"2","1":"Granting item permissions directly on the lakehouse, without assigning any workspace role, adheres to the principle of least privilege.","2":"Workspace \"Viewer\" role.","3":"Workspace \"Contributor\" role","4":"Item permissions only","5":"Workspace \"Admin\" role","6":"A data engineer needs access to only ONE lakehouse in a Fabric workspace. What's the MOST secure access method?","7":"91","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"Granular permissions in a lakehouse are applied using T-SQL Data Control Language (DCL) commands such as GRANT, DENY, and REVOKE.","2":"CREATE, ALTER, DROP","3":"GRANT, DENY, REVOKE","4":"SELECT, INSERT, UPDATE","5":"EXEC, CALL, DECLARE","6":"Which T-SQL commands are used to apply granular permissions in a lakehouse?","7":"92","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"OneLake serves as a hierarchical storage system that simplifies data management across an organization, built on Azure Data Lake Storage architecture.","2":"A type of data warehouse","3":"A hierarchical storage system built on Azure Data Lake Storage","4":"A collection of workspaces","5":"A dedicated resource for data science","6":"What is OneLake in Microsoft Fabric?","7":"93","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"2","1":"A domain is a logical grouping of workspaces used to organize items in a way that makes sense for an organization, such as grouping by sales, marketing, or finance.","2":"To define the capacity available for executing workloads","3":"To act as a container for items such as data warehouses and reports","4":"To logically group workspaces for organizational purposes","5":"To provide a single-pane-of-glass file-system namespace","6":"What is the role of a domain in Microsoft Fabric?","7":"94","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"1","1":"A workspace in Fabric acts as a container for items such as data warehouses, datasets, and reports, and provides controls for who can access these items.","2":"To provide a hierarchical file-system namespace","3":"To act as a container for items and manage access controls","4":"To define the capacity available for executing workloads","5":"To logically group workspaces for organizational purposes","6":"What is the primary function of a workspace in Microsoft Fabric?","7":"95","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"1","1":"The Fabric admin portal is specifically designed for managing all aspects of the Fabric platform.","2":"Microsoft 365 admin center","3":"Fabric admin portal","4":"PowerShell cmdlets","5":"Admin monitoring workspace","6":"What is the primary tool used by Fabric admins to manage the platform?","7":"96","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/3-admin-role-tools"},{"0":"1","1":"License management for Fabric is specifically handled in the Microsoft 365 admin center, which is designed for managing user licenses and access.","2":"In the Azure portal","3":"In the Microsoft 365 admin center","4":"In Power BI service","5":"In the Fabric management console","6":"Where is license management for Fabric handled?","7":"97","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/4-manage-security"},{"0":"2","1":"The first step in securing data is to grant the least permissive rights, which helps to minimize access and protect sensitive information.","2":"Implement sensitivity labels","3":"Conduct regular audits","4":"Granting the least permissive rights","5":"Enable multi-factor authentication","6":"What is the first step in securing data when managing sharing and distribution of items?","7":"98","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/4-manage-security"},{"0":"1","1":"The scanner API is specifically designed to enable admins to scan various Fabric items for sensitive data, enhancing data governance.","2":"Create new data warehouses","3":"Scan Fabric items for sensitive data","4":"Delete sensitive data from Fabric","5":"Promote content across the organization","6":"What does the scanner API in Fabric allow admins to do?","7":"99","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/5-govern-fabric"},{"0":"2","1":"Data lineage allows you to track the flow of data through Fabric, including its origin, transformations, and destinations, providing insights into how data is used.","2":"To endorse Fabric items as trusted","3":"To scan for sensitive data in Fabric items","4":"To track the flow of data through Fabric","5":"To promote content across the organization","6":"What is the primary purpose of data lineage in Microsoft Fabric?","7":"100","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/5-govern-fabric"}],"schema":[{"key":"0","name":"correctAnswer","type":"bigint"},{"key":"1","name":"explanation","type":"string"},{"key":"2","name":"option1","type":"string"},{"key":"3","name":"option2","type":"string"},{"key":"4","name":"option3","type":"string"},{"key":"5","name":"option4","type":"string"},{"key":"6","name":"question","type":"string"},{"key":"7","name":"sno","type":"bigint"},{"key":"8","name":"sourceUrl","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":null},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}},"78caf934-f8aa-4a0f-bc5a-f392ed79b7b3":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"Dataflows Gen2 are primarily used for transforming data within Microsoft Fabric, allowing for efficient data manipulation before it is stored or analyzed.","2":"Data ingestion","3":"Data transformation","4":"Data storage","5":"Data visualization","6":"What is the primary purpose of using Dataflows Gen2 in Microsoft Fabric?","7":"1","8":"https://learn.microsoft.com/en-us/training/modules/use-dataflow-gen-2-fabric/4-dataflow-pipeline"},{"0":"0","1":"Pipelines consist of two main types of activities: data transformation activities, which handle data transfer and transformation, and control flow activities, which manage the execution logic of the pipeline.","2":"Data transformation activities and Control flow activities","3":"Data processing activities and Control flow activities","4":"Data ingestion activities and Data storage activities","5":"Data extraction activities and Data transformation activities","6":"What are the two broad categories of activities in a pipeline?","7":"2","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/2-understand-fabric-pipeline"},{"0":"0","1":"Parameters allow pipelines to accept dynamic values (e.g., folder names) at runtime, enabling reusability.","2":"Parameters","3":"Variables","4":"Triggers","5":"Hardcoded folder names","6":"A team wants to reuse a pipeline to load data into different lakehouse folders dynamically. Which pipeline feature should they use?","7":"3","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/2-understand-fabric-pipeline"},{"0":"1","1":"The Copy Data activity is primarily used to ingest data from an external source into a lakehouse file or table without applying transformations.","2":"To apply transformations to data during ingestion","3":"To ingest data from an external source into a lakehouse file or table","4":"To delete existing data before copying new data","5":"To merge data from multiple sources","6":"What is the primary use of the Copy Data activity in a data pipeline?","7":"4","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"2","1":"A Data Flow activity should be used when you need to apply transformations to the data as it is ingested or merge data from multiple sources.","2":"When you need to copy data directly between a source and destination","3":"When you want to import raw data without transformations","4":"When you need to apply transformations to the data as it is ingested","5":"When you are using a graphical tool to configure the data source","6":"When should you consider using a Data Flow activity instead of a Copy Data activity?","7":"5","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"1","1":"Data Flow activities use Power Query for defining transformations.","2":"Azure Portal","3":"Power Query","4":"Spark Notebook","5":"Graphical Copy Data tool","6":"What tool is used to define transformation steps for a Data Flow activity?","7":"6","8":"https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/3-copy-data"},{"0":"1","1":"The lambda architecture combines the periodic loading of batch data for historical analysis with the ingestion of data streams for real-time analysis.","2":"Data warehouse architecture","3":"Lambda architecture","4":"Event-driven architecture","5":"Microservices architecture","6":"Which architecture combines batch data loading with real-time data ingestion?","7":"7","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"2","1":"Eventstream in Microsoft Fabric is used to ingest real-time data from diverse sources as a continuous stream.","2":"Eventhouse","3":"Power BI Dataset","4":"Eventstream","5":"KQL Database","6":"Which Microsoft Fabric component is specifically designed for ingesting continuous real-time data?","7":"8","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"1","1":"Activator enables defining automated alerts and actions when specific conditions occur in real-time data streams.","2":"Automate data transformation pipelines","3":"Trigger alerts based on streaming data thresholds","4":"Generate batch analytics reports","5":"Manage data warehouse indexing","6":"What is the primary purpose of Activator in Microsoft Fabric's Real-Time Intelligence?","7":"9","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The Join transformation is designed to combine data from two streams using a matching condition, unlike Union, which merges streams with shared fields.","2":"Join","3":"Union","4":"Group by","5":"Expand","6":"Which transformation in Eventstreams is used to combine data from two streams based on a matching condition between them?","7":"10","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"A KQL database is specifically designed as a real-time optimized data store that hosts a collection of tables, stored functions, materialized views, and shortcuts.","2":"A data store optimized for batch processing that hosts static tables","3":"A real-time optimized data store that hosts tables and functions","4":"A query language tool for processing historical data","5":"A storage system focused only on table management and data ingestion","6":"What is a KQL database?","7":"11","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"'take' is an operator in KQL that retrieves a specified number of rows from a table.","2":"Filters data based on a condition","3":"Retrieves a specified number of rows from a table","4":"Aggregates data","5":"Joins two tables together","6":"What does the 'take' operator do in a KQL query?","7":"12","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"Access to a Fabric lakehouse is managed through workspace roles for collaborators and item-level sharing for read-only needs.","2":"Through individual user accounts only","3":"Using workspace roles and item-level sharing","4":"Exclusively via API keys","5":"Through Azure Active Directory groups only","6":"How is access to a Fabric lakehouse typically managed?","7":"25","8":"https://learn.microsoft.com/en-us/training/modules/get-started-lakehouses/2-fabric-lakehouse"},{"0":"1","1":"The partitionBy method is used to optimize performance by partitioning the data into separate folders based on the specified column values.","2":"To specify the file format","3":"To optimize performance by partitioning data","4":"To define the schema","5":"To overwrite existing files","6":"What is the purpose of the partitionBy method when saving a DataFrame?","7":"26","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/4-dataframe"},{"0":"2","1":"The method df.write.format('delta').saveAsTable is specifically used to save a dataframe as a new table in Spark SQL.","2":"spark.catalog.createTable","3":"df.createOrReplaceTempView","4":"df.write.format('delta').saveAsTable","5":"spark.catalog.createExternalTable","6":"Which method is used to save a dataframe as a new table in Spark SQL?","7":"27","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql"},{"0":"1","1":"Deleting an external table only removes the metadata from the catalog, while the underlying data remains intact.","2":"Both the metadata and data files are deleted","3":"Only the metadata is deleted from the catalog, while the data files remain intact in their storage location","4":"The metadata is archived and the data files are deleted","5":"The metadata and data files are both archived, and can be restored later","6":"What happens when you delete an external table from Spark Catalog?","7":"28","8":"https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql"},{"0":"1","1":"Delta tables provide ACID transaction support, ensuring that data modifications are atomic, consistent, isolated, and durable.","2":"They only support batch data processing.","3":"They provide ACID transaction support.","4":"They can only store static data.","5":"They do not support data versioning.","6":"What is the primary benefit of Delta tables in Microsoft Fabric lakehouses?","7":"29","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/2-understand-delta-lake"},{"0":"1","1":"The _delta_log folder contains transaction logs in JSON format, which are used to track changes and ensure ACID compliance.","2":"To store Parquet data files","3":"To log transaction details in JSON format","4":"To store metadata for unstructured data","5":"To cache query results for faster access","6":"What is the purpose of the _delta_log folder in a Delta table?","7":"30","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/2-understand-delta-lake"},{"0":"1","1":"When a managed table is deleted, both the table definition and the underlying data files are deleted from the lakehouse.","2":"They are retained","3":"They are deleted","4":"They are archived","5":"They are moved to another location","6":"What happens to the underlying data files when a managed table is deleted in a Fabric lakehouse?","7":"31","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3-create-delta-tables"},{"0":"1","1":"OptimizeWrite helps in writing fewer larger files instead of many small files, addressing the small file problem.","2":"To delete old data files","3":"To reduce the number of small files written","4":"To partition data into subfolders","5":"To enable time travel for data","6":"What is the purpose of the OptimizeWrite function in Delta Lake?","7":"32","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3b-optimize-delta-tables"},{"0":"1","1":"The VACUUM command is used to remove old data files that are not referenced in the transaction log, helping to manage storage.","2":"It consolidates small files into larger ones","3":"It removes old data files that are no longer referenced","4":"It enables faster reads from the compute engines","5":"It partitions data into subfolders","6":"What does the VACUUM command do in Delta Lake?","7":"33","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/3b-optimize-delta-tables"},{"0":"1","1":"The DESCRIBE HISTORY command is specifically used to view the history of transactions applied to a delta table.","2":"SHOW HISTORY","3":"DESCRIBE HISTORY","4":"VIEW HISTORY","5":"GET HISTORY","6":"Which command is used to view the history of changes made to a delta table?","7":"34","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/4-work-delta-data"},{"0":"0","1":"To retrieve data from a specific version of a delta table, you can use the versionAsOf option when reading the delta file location into a dataframe.","2":"Using the versionAsOf option","3":"Using the timestampAsOf option","4":"Using the version option","5":"Using the timeTravel option","6":"How can you retrieve data from a specific version of a delta table?","7":"35","8":"https://learn.microsoft.com/en-us/training/modules/work-delta-lake-tables-fabric/4-work-delta-data"},{"0":"2","1":"The bronze layer serves as the initial landing zone for all data, where it is stored in its original format without any changes.","2":"To store data in a refined format","3":"To validate and clean the data","4":"To serve as the landing zone for raw data","5":"To aggregate data for analytics","6":"What is the primary purpose of the bronze layer in the medallion architecture?","7":"36","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/2-describe-medallion-architecture"},{"0":"2","1":"Tumbling windows are characterized by dividing events into fixed and nonoverlapping intervals based on their arrival time.","2":"Sliding windows","3":"Session windows","4":"Tumbling windows","5":"Hopping windows","6":"Which window type divides incoming events into fixed and nonoverlapping intervals?","7":"49","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"1","1":"The Union transformation is used to connect multiple nodes and combine events that share the same fields, effectively merging data streams.","2":"To filter events based on specific criteria","3":"To connect two or more nodes and combine events with shared fields","4":"To calculate aggregations over a specified time period","5":"To create new rows for each value in an array","6":"What is the function of the Union transformation in eventstream processing?","7":"50","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"2","1":"The grouping key in the Group by transformation specifies one or more columns in the event data to group events by, such as sensor ID or item category.","2":"To define the length of each window interval","3":"To shift the start and end of each window interval","4":"To specify the columns to group events by","5":"To filter events based on a condition","6":"What is the purpose of the grouping key in the Group by transformation?","7":"51","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"0","1":"Creating an eventhouse requires a workspace with a Fabric capacity that supports the Real-Time Intelligence Fabric capability.","2":"A Fabric capacity that supports Real-Time Intelligence Fabric capability","3":"A KQL database already created","4":"A sample dataset imported","5":"An Azure Event Hub configured","6":"What is required to create an eventhouse in Microsoft Fabric?","7":"52","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"The KQL query 'Automotive | take 100' is specifically designed to retrieve a sample of 100 rows from the Automotive table.","2":"SELECT * FROM Automotive LIMIT 100","3":"Automotive | take 100","4":"Automotive | limit 100","5":"SELECT TOP 100 * FROM Automotive","6":"Which KQL query retrieves the first 100 rows from the Automotive table?","7":"53","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"KQL Querysets are designed to simplify query development by providing sample syntax and coding utilities for users.","2":"To store KQL databases","3":"To simplify query development with sample syntax and coding utilities","4":"To visualize query results in Power BI","5":"To import data from static locations","6":"What is the purpose of KQL Querysets in eventhouses?","7":"54","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/2-get-started-with-kql-queries"},{"0":"1","1":"A real-time dashboard requires a source of real-time data to function effectively.","2":"A static data source","3":"A source of real-time data","4":"A SQL database","5":"A data warehouse","6":"What is required to create a real-time dashboard in Microsoft Fabric?","7":"55","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/2-get-started-with-real-time-dashboards"},{"0":"1","1":"The 'Dashboard editor's identity' scheme uses the identity of the user who created the dashboard to access data.","2":"Pass-through identity","3":"Dashboard editor's identity","4":"User-specific identity","5":"Admin identity","6":"Which authorization scheme allows the dashboard to access data using the identity of the user who created the dashboard?","7":"56","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/2-get-started-with-real-time-dashboards"},{"0":"0","1":"A base query allows for the retrieval of a general set of records that can be used across multiple tiles, making the dashboard more maintainable.","2":"To retrieve a general set of records relevant for multiple tiles","3":"To create a static dashboard without any data updates","4":"To limit the number of tiles on the dashboard","5":"To enhance the visual appeal of the dashboard","6":"What is the purpose of defining a base query in a real-time dashboard?","7":"57","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"0","1":"Parameters provide flexibility by enabling users to filter the data displayed in the tiles based on their selections.","2":"By allowing users to filter data displayed in the tiles","3":"By automatically refreshing the dashboard every minute","4":"By limiting the number of pages in the dashboard","5":"By providing a static view of the data","6":"How can parameters enhance a real-time dashboard?","7":"58","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"0","1":"The auto refresh feature ensures that the dashboard data is updated automatically, eliminating the need for manual refresh actions.","2":"It automatically updates dashboard data without manual intervention","3":"It prevents any changes to the dashboard layout","4":"It allows users to set a maximum refresh rate","5":"It disables all other dashboard features","6":"What does the 'auto refresh' feature do in a real-time dashboard?","7":"59","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/3-advanced-features"},{"0":"1","1":"Clarity and simplicity are essential to ensure that dashboards are easy to understand and use, avoiding clutter.","2":"Complexity and Detail","3":"Clarity and Simplicity","4":"High Refresh Rates","5":"Limited Interactivity","6":"What is a key principle to follow when designing real-time dashboards in Microsoft Fabric?","7":"60","8":"https://learn.microsoft.com/en-us/training/modules/create-real-time-dashboards-microsoft-fabric/4-real-time-dashboards-best-practices"},{"0":"1","1":"The star schema is defined by its structure where a central fact table is directly connected to multiple dimension tables, facilitating efficient queries.","2":"Snowflake schema","3":"Star schema","4":"Galaxy schema","5":"Hybrid schema","6":"Which schema design is characterized by a fact table directly related to dimension tables?","7":"61","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/2-understand-data-warehouse"},{"0":"1","1":"Surrogate keys serve as unique identifiers for each row in a dimension table, ensuring consistency and accuracy within the data warehouse.","2":"To provide a natural identifier from the source system","3":"To uniquely identify each row in the dimension table","4":"To track changes in dimension attributes over time","5":"To aggregate data over temporal intervals","6":"What is the role of surrogate keys in dimension tables?","7":"62","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/2-understand-data-warehouse"},{"0":"2","1":"Zero-copy table clones allow for minimal storage costs because they reference the same underlying data files without duplicating them.","2":"Increased storage costs","3":"Faster data ingestion","4":"Minimal storage costs while referencing the same data","5":"Automatic data cleansing","6":"Which of the following is a benefit of using zero-copy table clones in a data warehouse?","7":"63","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/3-understand-data-warehouse-fabric"},{"0":"0","1":"A semantic model is designed to define relationships between tables, aggregation rules, and calculations for deriving insights from data.","2":"To define relationships and calculations for data insights","3":"To store raw data without any transformations","4":"To visualize data without any reporting tools","5":"To manage user permissions in the data warehouse","6":"What is the purpose of a semantic model in a data warehouse?","7":"64","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/5-model-data"},{"0":"2","1":"The sys.dm_exec_requests DMV provides details about each active request in a session, allowing for monitoring of ongoing operations.","2":"sys.dm_exec_connections","3":"sys.dm_exec_sessions","4":"sys.dm_exec_requests","5":"sys.dm_exec_queries","6":"Which dynamic management view (DMV) would you use to get information about active requests in a session?","7":"65","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/6-security-monitor"},{"0":"2","1":"The Read permission is essential for establishing a connection to the SQL analytics endpoint, as it allows the user to connect using the SQL connection string.","2":"ReadData","3":"ReadAll","4":"Read","5":"Write","6":"What permission must a user have at a minimum to connect to the SQL analytics endpoint?","7":"66","8":"https://learn.microsoft.com/en-us/training/modules/get-started-data-warehouse/6-security-monitor"},{"0":"2","1":"Type 2 SCD adds new records for changes and keeps full history for a given natural key, allowing for historical analysis.","2":"Type 0 SCD","3":"Type 1 SCD","4":"Type 2 SCD","5":"Type 3 SCD","6":"Which type of slowly changing dimension (SCD) keeps full history for a given natural key?","7":"67","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/2-explore-data-load-strategies"},{"0":"1","1":"The REJECTED_ROW_LOCATION option allows for better error handling by storing rows that were not successfully imported.","2":"To specify the format of the source file","3":"To store rejected rows separately","4":"To skip header rows","5":"To define the target table","6":"What is the purpose of the REJECTED_ROW_LOCATION option in the COPY statement?","7":"68","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"2","1":"The INSERT...SELECT operation is used to insert data from one table into another without creating a new table.","2":"CREATE TABLE AS SELECT","3":"COPY INTO","4":"INSERT...SELECT","5":"BULK INSERT","6":"Which SQL operation is used to insert data from one table into another without creating a new table?","7":"69","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"1","1":"The three-part naming convention allows combining data from a warehouse and a lakehouse by referencing tables across these assets.","2":"COPY statement","3":"Three-part naming convention","4":"Shared Access Signature","5":"Storage Account Key","6":"Which feature allows combining data from a warehouse and a lakehouse in Microsoft Fabric?","7":"70","8":"https://learn.microsoft.com/en-us/training/modules/load-data-into-microsoft-fabric-data-warehouse/4-load-data-using-tsql"},{"0":"1","1":"The APPROX_COUNT_DISTINCT function is specifically designed to retrieve an approximate count of distinct values using the HyperLogLog algorithm, which is useful for large datasets where an exact count is not required.","2":"COUNT","3":"APPROX_COUNT_DISTINCT","4":"DISTINCT_COUNT","5":"ESTIMATE_COUNT","6":"Which T-SQL function is used to retrieve an approximate count of distinct values in a data warehouse?","7":"71","8":"https://learn.microsoft.com/en-us/training/modules/query-data-warehouse-microsoft-fabric/2-query-data"},{"0":"2","1":"The DENSE_RANK function assigns the same rank to rows with the same values and does not skip subsequent ranking positions, unlike RANK, which does skip positions after ties.","2":"ROW_NUMBER","3":"RANK","4":"DENSE_RANK","5":"NTILE","6":"Which T-SQL ranking function assigns the same rank to rows with the same values but does not skip subsequent ranking positions?","7":"72","8":"https://learn.microsoft.com/en-us/training/modules/query-data-warehouse-microsoft-fabric/2-query-data"},{"0":"1","1":"The 'SELECT TOP 10 * FROM stock' is the SQL equivalent of the KQL 'take 10' operator.","2":"SELECT * FROM stock LIMIT 10","3":"SELECT TOP 10 * FROM stock","4":"FETCH FIRST 10 ROWS FROM stock","5":"SELECT * FROM stock WHERE ROWNUM <= 10","6":"Which T-SQL statement is equivalent to the KQL query 'stock | take 10'?","7":"13","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"2","1":"Transformations can perform a variety of operations including filtering, joining, aggregating, and grouping, as well as temporal windowing functions.","2":"Only filtering data","3":"Only aggregating data","4":"Filtering, joining, aggregating, and grouping","5":"Storing data in destinations","6":"What type of operations can transformations in an eventstream perform?","7":"14","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/2-eventstream-components"},{"0":"2","1":"The Derived Stream destination is created after applying stream operations like Filter or Manage Fields to an eventstream, representing the altered default stream.","2":"To store raw event data in a KQL database","3":"To preprocess events before ingestion into a lakehouse","4":"To represent the altered default stream after applying stream operations","5":"To trigger automated actions based on streaming data values","6":"What is the primary purpose of the Derived Stream destination in Microsoft Fabric Eventstreams?","7":"15","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/3-setup-eventstreams"},{"0":"2","1":"Session windows group events into variable and non-overlapping intervals based on a gap of inactivity.","2":"Tumbling","3":"Sliding","4":"Session","5":"Hopping","6":"Which window type groups events based on a gap of inactivity?","7":"16","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/4-route-event-data-to-destinations"},{"0":"2","1":"The 'project' keyword is used in KQL to specify which columns to retrieve from a dataset, helping to optimize performance by minimizing the data returned.","2":"filter","3":"select","4":"project","5":"retrieve","6":"What keyword should you use to retrieve only specific columns in a KQL query?","7":"17","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/3-kql-best-practices"},{"0":"1","1":"The ingestion_time() function filters data based on when it was loaded into the table, which is essential when the data itself lacks a timestamp.","2":"Automotive | where pickup_datetime > ago(1h)","3":"Automotive | where ingestion_time() > ago(1h)","4":"Automotive | where current_utc_time() - ingestion_time() < 1h","5":"Automotive | where timestamp > ago(1h)","6":"Which KQL query correctly retrieves events ingested into the Automotive table within the past hour when the data lacks a timestamp?","7":"18","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/3-kql-best-practices"},{"0":"1","1":"Materialized views created without the backfill option are populated incrementally as new data is ingested, rather than processing existing data.","2":"The view is immediately populated with all existing data.","3":"The view is populated incrementally as new data is ingested.","4":"Existing data is deleted from the source table.","5":"The view creation fails unless backfill is enabled.","6":"How does a materialized view populate itself when created without the backfill option?","7":"19","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/4-advanced-features"},{"0":"2","1":"Materialized views are designed to maintain pre-aggregated datasets efficiently, which is ideal for frequent queries requiring summarized data.","2":"Reusing a parameterized query with variable inputs","3":"Creating an on-demand summary of historical data","4":"Automatically maintaining a pre-aggregated dataset for frequent queries","5":"Performing real-time transformations during data ingestion","6":"Which scenario best justifies using a materialized view instead of a stored function?","7":"20","8":"https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/4-advanced-features"},{"0":"0","1":"OneLake is designed to provide a single, integrated storage environment for all analytics engines in Fabric, eliminating the need to move or copy data between systems.","2":"To provide a unified storage solution for all analytics engines in Fabric","3":"To act as a data integration tool for moving data between systems","4":"To serve as a standalone data warehouse solution","5":"To manage compute resources for Fabric workspaces","6":"What is the primary purpose of OneLake in Microsoft Fabric?","7":"21","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/2-explore-analytics-fabric"},{"0":"1","1":"Shortcuts in OneLake enable users to create embedded references to existing data sources, facilitating easy access without duplication.","2":"Data lineage","3":"Shortcuts","4":"Dataflows","5":"Workspaces","6":"What feature of OneLake allows users to reference existing cloud data without copying it?","7":"22","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/2-explore-analytics-fabric"},{"0":"1","1":"A workspace serves as a collaborative environment for creating and managing items like lakehouses, warehouses, and reports.","2":"To store data permanently","3":"To create and manage collaborative items","4":"To perform data analysis","5":"To configure Azure settings","6":"What is the primary purpose of a workspace in Microsoft Fabric?","7":"23","8":"https://learn.microsoft.com/en-us/training/modules/introduction-end-analytics-use-microsoft-fabric/4-use-fabric"},{"0":"1","1":"Lakehouses support ACID transactions, which are essential for maintaining data consistency and integrity.","2":"Schema-on-write","3":"ACID transactions through Delta Lake formatted tables","4":"Only read-only access","5":"Data replication across multiple regions","6":"What feature do lakehouses support to ensure data consistency and integrity?","7":"24","8":"https://learn.microsoft.com/en-us/training/modules/get-started-lakehouses/2-fabric-lakehouse"},{"0":"1","1":"The medallion architecture ensures data reliability and consistency as it moves through different layers, improving data quality and making it easier to analyze.","2":"It replaces existing data models entirely","3":"It ensures data reliability and consistency as it moves through layers","4":"It eliminates the need for data transformation tools","5":"It restricts data access to a single team","6":"What is the primary benefit of using the medallion architecture in a lakehouse?","7":"37","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/2-describe-medallion-architecture"},{"0":"2","1":"The Gold layer is modeled in a star schema, which is optimized for reporting.","2":"Bronze","3":"Silver","4":"Gold","5":"Raw","6":"Which layer in the medallion architecture is typically modeled in a star schema?","7":"38","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/3-implement-medallion-archecture-fabric"},{"0":"1","1":"The SQL analytics endpoint allows users to write queries, manage the semantic model, and utilize a visual query experience.","2":"To create new data layers in the medallion architecture","3":"To write queries, manage the semantic model, and query data visually","4":"To generate cleansed data for third-party applications","5":"To connect directly to Power BI without a semantic model","6":"What is the purpose of the SQL analytics endpoint in Fabric?","7":"39","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/4-query-report-data"},{"0":"2","1":"Direct Lake mode caches often-used data and refreshes it as required, enhancing performance and ensuring up-to-date access to lakehouse data.","2":"It allows for real-time data processing without caching","3":"It provides a static view of the data in the lakehouse","4":"It caches frequently used data and refreshes it as needed","5":"It requires manual updates to the semantic model","6":"How does Direct Lake mode benefit data analysts using Power BI?","7":"40","8":"https://learn.microsoft.com/en-us/training/modules/describe-medallion-architecture/4-query-report-data"},{"0":"1","1":"A key characteristic of a data stream in real-time analytics is that it is unbounded, meaning data is continuously added to the stream without a predefined limit.","2":"Data records are fixed and do not change","3":"Data is added to the stream perpetually","4":"Data streams are only processed once a day","5":"Data records do not include time-based information","6":"Which characteristic describes a data stream in real-time analytics?","7":"41","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The results of streaming data processing can be used to support real-time (or near real-time) automation or visualization, enabling immediate insights and actions.","2":"To support real-time automation or visualization.","3":"To archive data in a cold storage solution.","4":"To perform batch processing of historical data.","5":"To generate static reports for regulatory compliance.","6":"How can the results of streaming data processing be used?","7":"42","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/2-define-real-time-analytics"},{"0":"0","1":"The Microsoft Fabric Real-Time Intelligence solution is designed to provide an end-to-end streaming solution for real-time data analysis, including managing real-time data sources.","2":"To provide a centralized place for managing real-time data sources","3":"To create and manage virtual machines in Azure","4":"To deploy highly available web applications","5":"To store and analyze batch data in a data warehouse","6":"What is the primary purpose of the Microsoft Fabric Real-Time Intelligence solution?","7":"43","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3-describe-kusto-databases-tables"},{"0":"2","1":"The Eventhouse destination is specifically meant for ingesting real-time event data, allowing for analysis using Kusto Query Language.","2":"Lakehouse","3":"Derived stream","4":"Eventhouse","5":"Fabric Activator","6":"Which Eventstreams destination allows direct KQL analysis of raw event data?","7":"44","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"The Union transformation is used to connect multiple nodes and combine events with shared fields into one table.","2":"Groups array values into a single row","3":"Creates a new row for each value within an array","4":"Removes array values from the stream","5":"Combines multiple arrays into one array","6":"What does the Expand transformation do in eventstreams?","7":"45","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/3a-define-real-time-hub"},{"0":"1","1":"A KQL database is designed to host tables and functions that facilitate real-time data analysis.","2":"Storing static data","3":"Hosting a collection of tables and functions for real-time data analysis","4":"Performing batch processing of data","5":"Creating visual reports","6":"What is a KQL database primarily used for in an eventhouse?","7":"46","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4-write-queries-kusto-query-language"},{"0":"1","1":"Rules are crucial in Activator as they define the conditions under which actions are triggered based on property values.","2":"They define the structure of events","3":"They set conditions for triggering actions","4":"They represent business objects","5":"They map properties to event data","6":"What role do Rules play in Activator?","7":"47","8":"https://learn.microsoft.com/en-us/training/modules/get-started-kusto-fabric/4c-activator"},{"0":"1","1":"Microsoft Fabric eventstreams are designed to facilitate the movement of event data from various sources to different destinations, functioning like a conveyor belt.","2":"To store data in a database","3":"To create a pipeline of events from sources to destinations","4":"To manage infrastructure for applications","5":"To write complex code for data processing","6":"What is the primary function of Microsoft Fabric eventstreams?","7":"48","8":"https://learn.microsoft.com/en-us/training/modules/explore-event-streams-microsoft-fabric/2-eventstream-components"},{"0":"1","1":"Throttling in Microsoft Fabric indicates that the processes require more capacity than is available within the constraints of the purchased capacity license.","2":"The system is running at optimal capacity","3":"The processes require more capacity than is available","4":"The data warehouse is being migrated to a new region","5":"The license has expired and needs renewal","6":"What does throttling in Microsoft Fabric indicate?","7":"73","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/02-capacity-metrics"},{"0":"1","1":"The sys.dm_exec_connections DMV specifically provides information regarding the connections to the data warehouse.","2":"Active requests","3":"Data warehouse connections","4":"Authenticated sessions","5":"Database transactions","6":"What does the sys.dm_exec_connections DMV return information about?","7":"74","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/03-dynamic-management-views"},{"0":"2","1":"The queryinsights.long_running_queries view is specifically designed to provide aggregated data regarding query execution time, including the median execution time (median_total_elapsed_time_ms).","2":"queryinsights.exec_requests_history","3":"queryinsights.frequently_run_queries","4":"queryinsights.long_running_queries","5":"A calculated field derived from queryinsights.exec_requests_history only","6":"Which queryinsights view directly provides the median execution time for SQL commands?","7":"75","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-data-warehouse/04-query-insights"},{"0":"1","1":"The primary benefit of DDM is its ability to mask sensitive data in real time, ensuring that unauthorized users do not see the actual data.","2":"It permanently alters the data in the database","3":"It allows real-time masking of sensitive data","4":"It requires complex coding to implement","5":"It exposes all data to nonprivileged users","6":"What is the primary benefit of Dynamic Data Masking (DDM)?","7":"76","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"The email() function is specifically designed to expose the first letter of an email address while masking the rest.","2":"default()","3":"email()","4":"partial(prefix_padding, padding_string, suffix_padding)","5":"random(low, high)","6":"Which masking function would you use to partially hide an email address while exposing the first letter?","7":"77","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"The correct command to apply a masking rule to the PhoneNumber column is to use the partial() function with the specified parameters.","2":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'random(1000000000,9999999999)')","3":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXX-XXX-\",4)')","4":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'email()')","5":"ALTER TABLE Customers ALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'default()')","6":"You need to dynamically mask the PhoneNumber column to display only the last four digits, replacing the rest with XXX-XXX-. Which SQL command achieves this?","7":"78","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/2-explore-dynamic-data-masking"},{"0":"1","1":"Column-level security is designed to restrict access to sensitive data, ensuring that only authorized users can view specific columns.","2":"To enhance performance of queries","3":"To restrict access to sensitive data","4":"To simplify database management","5":"To allow all users to access all data","6":"What is the primary purpose of column-level security (CLS) in a data warehouse?","7":"79","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/4-implement-column-level-security"},{"0":"1","1":"Column-level security automatically adapts to changes in table structure, making it easier to manage than views which may require updates when the underlying structure changes.","2":"More flexible in defining permissions","3":"Automatically adapts to table structure changes","4":"Can provide row-level security","5":"Requires less maintenance","6":"What is a key advantage of using column-level security compared to views?","7":"80","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-in-microsoft-fabric/4-implement-column-level-security"},{"0":"1","1":"Deployment pipelines facilitate the promotion of code changes through various environments, ensuring a structured deployment process.","2":"They automate the integration of code changes","3":"They allow for the promotion of code changes to different environments","4":"They manage version control for individual developers","5":"They provide a platform for manual testing of code","6":"What role do deployment pipelines play in the CI/CD process within Fabric?","7":"81","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/2-understand-cicd"},{"0":"1","1":"Continuous delivery follows continuous integration and involves deploying code to a staging environment for additional automated testing before production release.","2":"It automatically releases updates into production without any testing","3":"It occurs after continuous integration and involves deploying code to a staging environment for further testing","4":"It is the process of merging code changes from multiple developers into a single branch","5":"It focuses solely on the management of version control using Git","6":"Which of the following statements best describes continuous delivery?","7":"82","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/2-understand-cicd"},{"0":"1","1":"Deployment pipelines are designed to automate the movement of content through various stages of the development lifecycle, ensuring efficient updates and testing.","2":"To create user interfaces for applications","3":"To automate the movement of content through development stages","4":"To manage user permissions across environments","5":"To store data securely in the cloud","6":"What is the primary purpose of deployment pipelines in Microsoft Fabric?","7":"83","8":"https://learn.microsoft.com/en-us/training/modules/implement-cicd-in-fabric/4-implement-deployment-pipelines"},{"0":"1","1":"Monitoring is essential for collecting data and metrics that help determine the health and operational status of a system.","2":"To enhance user interface design","3":"To collect system data and metrics for health assessment","4":"To automate data ingestion processes","5":"To create visual representations of data models","6":"What is the primary purpose of monitoring in Microsoft Fabric?","7":"84","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/2-understand-monitoring"},{"0":"1","1":"The Monitor hub is designed specifically to visualize and monitor various activities within Microsoft Fabric, making it easier to track performance and issues.","2":"To create new data pipelines","3":"To visualize and monitor Fabric activities","4":"To store data permanently","5":"To execute Spark jobs","6":"What is the primary function of the Monitor hub in Microsoft Fabric?","7":"85","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/3-use-monitor-hub"},{"0":"1","1":"Activator is designed to automate actions triggered by specific events in data streams.","2":"To store large volumes of data","3":"To automate actions based on events","4":"To visualize data in real-time dashboards","5":"To manage user access and permissions","6":"What is the primary function of Activator in Microsoft Fabric?","7":"86","8":"https://learn.microsoft.com/en-us/training/modules/monitor-fabric-items/4-take-action-with-activator"},{"0":"1","1":"Workspace roles in Fabric are used to distribute ownership and access policies within a workspace, enabling control over who can access and manage resources.","2":"To authenticate users via Microsoft Entra ID","3":"To distribute ownership and access policies within a workspace","4":"To apply granular permissions within a specific compute engine","5":"To restrict access to specific files or folders in OneLake","6":"In Fabric, what is the purpose of workspace roles?","7":"87","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/2-understand-fabric-security-model"},{"0":"2","1":"OneLake data access controls enable the restriction of access to specific files or folders within the lakehouse.","2":"Workspace roles","3":"Item permissions","4":"OneLake data access controls","5":"Granular engine permissions","6":"What feature allows access to data in the lakehouse to be restricted to specific files or folders?","7":"88","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/2-understand-fabric-security-model"},{"0":"2","1":"The Contributor role allows users to create Fabric items and modify content, which meets the requirements for the data engineer.","2":"Admin","3":"Member","4":"Contributor","5":"Viewer","6":"What role should be assigned to a data engineer who needs to create Fabric items and read data in a lakehouse?","7":"89","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"The GRANT command is specifically used in SQL to provide permissions to users or roles for accessing database objects.","2":"ALLOW","3":"GRANT","4":"PERMIT","5":"ENABLE","6":"What command is used to grant permissions to SQL objects using the SQL analytics endpoint?","7":"90","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"2","1":"Granting item permissions directly on the lakehouse, without assigning any workspace role, adheres to the principle of least privilege.","2":"Workspace \"Viewer\" role.","3":"Workspace \"Contributor\" role","4":"Item permissions only","5":"Workspace \"Admin\" role","6":"A data engineer needs access to only ONE lakehouse in a Fabric workspace. What's the MOST secure access method?","7":"91","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"Granular permissions in a lakehouse are applied using T-SQL Data Control Language (DCL) commands such as GRANT, DENY, and REVOKE.","2":"CREATE, ALTER, DROP","3":"GRANT, DENY, REVOKE","4":"SELECT, INSERT, UPDATE","5":"EXEC, CALL, DECLARE","6":"Which T-SQL commands are used to apply granular permissions in a lakehouse?","7":"92","8":"https://learn.microsoft.com/en-us/training/modules/secure-data-access-in-fabric/3-configure-workspace-and-item-permissions"},{"0":"1","1":"OneLake serves as a hierarchical storage system that simplifies data management across an organization, built on Azure Data Lake Storage architecture.","2":"A type of data warehouse","3":"A hierarchical storage system built on Azure Data Lake Storage","4":"A collection of workspaces","5":"A dedicated resource for data science","6":"What is OneLake in Microsoft Fabric?","7":"93","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"2","1":"A domain is a logical grouping of workspaces used to organize items in a way that makes sense for an organization, such as grouping by sales, marketing, or finance.","2":"To define the capacity available for executing workloads","3":"To act as a container for items such as data warehouses and reports","4":"To logically group workspaces for organizational purposes","5":"To provide a single-pane-of-glass file-system namespace","6":"What is the role of a domain in Microsoft Fabric?","7":"94","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"1","1":"A workspace in Fabric acts as a container for items such as data warehouses, datasets, and reports, and provides controls for who can access these items.","2":"To provide a hierarchical file-system namespace","3":"To act as a container for items and manage access controls","4":"To define the capacity available for executing workloads","5":"To logically group workspaces for organizational purposes","6":"What is the primary function of a workspace in Microsoft Fabric?","7":"95","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/2-fabric-architecture"},{"0":"1","1":"The Fabric admin portal is specifically designed for managing all aspects of the Fabric platform.","2":"Microsoft 365 admin center","3":"Fabric admin portal","4":"PowerShell cmdlets","5":"Admin monitoring workspace","6":"What is the primary tool used by Fabric admins to manage the platform?","7":"96","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/3-admin-role-tools"},{"0":"1","1":"License management for Fabric is specifically handled in the Microsoft 365 admin center, which is designed for managing user licenses and access.","2":"In the Azure portal","3":"In the Microsoft 365 admin center","4":"In Power BI service","5":"In the Fabric management console","6":"Where is license management for Fabric handled?","7":"97","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/4-manage-security"},{"0":"2","1":"The first step in securing data is to grant the least permissive rights, which helps to minimize access and protect sensitive information.","2":"Implement sensitivity labels","3":"Conduct regular audits","4":"Granting the least permissive rights","5":"Enable multi-factor authentication","6":"What is the first step in securing data when managing sharing and distribution of items?","7":"98","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/4-manage-security"},{"0":"1","1":"The scanner API is specifically designed to enable admins to scan various Fabric items for sensitive data, enhancing data governance.","2":"Create new data warehouses","3":"Scan Fabric items for sensitive data","4":"Delete sensitive data from Fabric","5":"Promote content across the organization","6":"What does the scanner API in Fabric allow admins to do?","7":"99","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/5-govern-fabric"},{"0":"2","1":"Data lineage allows you to track the flow of data through Fabric, including its origin, transformations, and destinations, providing insights into how data is used.","2":"To endorse Fabric items as trusted","3":"To scan for sensitive data in Fabric items","4":"To track the flow of data through Fabric","5":"To promote content across the organization","6":"What is the primary purpose of data lineage in Microsoft Fabric?","7":"100","8":"https://learn.microsoft.com/en-us/training/modules/administer-fabric/5-govern-fabric"}],"schema":[{"key":"0","name":"correctAnswer","type":"bigint"},{"key":"1","name":"explanation","type":"string"},{"key":"2","name":"option1","type":"string"},{"key":"3","name":"option2","type":"string"},{"key":"4","name":"option3","type":"string"},{"key":"5","name":"option4","type":"string"},{"key":"6","name":"question","type":"string"},{"key":"7","name":"sno","type":"bigint"},{"key":"8","name":"sourceUrl","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"dependencies":{"lakehouse":{"default_lakehouse":"6d3e28c5-c089-49b5-ba0d-c725bbcf2865","default_lakehouse_name":"BronzeLH","default_lakehouse_workspace_id":"7c87f734-583b-4e85-ab3f-dc1b967ba95b"}}},"nbformat":4,"nbformat_minor":5}